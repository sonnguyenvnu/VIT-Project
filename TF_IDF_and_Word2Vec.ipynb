{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q16_onj8yBaL"
   },
   "outputs": [],
   "source": [
    "!pip install lime gensim keras transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0vPrhopYxwTJ"
   },
   "outputs": [],
   "source": [
    "## for data\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "## for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "## for processing\n",
    "import re\n",
    "# import nltk\n",
    "## for bag-of-words\n",
    "from sklearn import feature_extraction, model_selection, naive_bayes, pipeline, manifold, preprocessing\n",
    "## for explainer\n",
    "from lime import lime_text\n",
    "## for word embedding\n",
    "import gensim\n",
    "import gensim.downloader as gensim_api\n",
    "## for deep learning\n",
    "from tensorflow.keras import models, layers, preprocessing as kprocessing\n",
    "from tensorflow.keras import backend as K\n",
    "## for bert language model\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3hfASDNSjuqP"
   },
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cPtpRnYIx_JZ"
   },
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv(\"YOUR_TRAIN_PATH\", sep='#')\n",
    "test_raw = pd.read_csv(\"YOUR_TEST_PATH\", sep='#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GASE-UPCzmHn"
   },
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "  removed_cols = ['Unnamed: 0',\n",
    "                  'Unnamed: 0.1',\n",
    " 'CVE Page',\n",
    " 'Summary',\n",
    " 'Vulnerability Classification',\n",
    " 'codeLink',\n",
    " 'commit_id',\n",
    " 'commit_message',\n",
    " 'del_lines',\n",
    " 'file_name',\n",
    " 'func_before',\n",
    " 'vul_func_with_fix',\n",
    " 'flaw_line',\n",
    " 'flaw_line_index',\n",
    " 'vul_func_with_fix',\n",
    " 'processed_func',\n",
    " 'sql',\n",
    " 'r.spl.',\n",
    " 'dir.',\n",
    " 'trav.',\n",
    " 'http',\n",
    " 'xss',\n",
    " 'corr.',\n",
    " 'cpg']\n",
    "  \n",
    "  try:\n",
    "      df = df.rename({'processed_func' : 'text', '+info': 'info', '+priv': 'priv', 'mem.' : 'mem'}, axis=1, inplace=False)\n",
    "  except:\n",
    "      print(\"RENAMED\")\n",
    "\n",
    "  for c in removed_cols:\n",
    "    try:\n",
    "      df = df.drop(c, axis=1)\n",
    "    except:\n",
    "      print(c)\n",
    "  df = df.drop('code', axis=1)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cNcd02WQzogY"
   },
   "outputs": [],
   "source": [
    "train_df = preprocess(train_raw)\n",
    "test_df = preprocess(test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z8DlU18ttnjS"
   },
   "outputs": [],
   "source": [
    "def hamming_score(preds, lbs):\n",
    "    sum_ratio = 0.0\n",
    "    for i, row in preds.iterrows():\n",
    "        upper = 0\n",
    "        lower = 0\n",
    "        ratio = 0.0\n",
    "        for lb in LABEL_COLUMNS:\n",
    "            logic_and = preds[lb][i] + lbs[lb][i]\n",
    "            if logic_and == 2:\n",
    "                upper = upper+1\n",
    "                lower = lower+1\n",
    "            if logic_and == 1:\n",
    "                lower = lower+1\n",
    "        if lower != 0:\n",
    "            ratio = upper/lower\n",
    "        sum_ratio = sum_ratio+ratio\n",
    "    return sum_ratio/len(preds)\n",
    "\n",
    "def exact_match_ratio(preds, lbs):\n",
    "    sum_ratio = 0.0\n",
    "    for i, row in preds.iterrows():\n",
    "        upper = 0\n",
    "        ratio = 0.0\n",
    "        for lb in LABEL_COLUMNS:\n",
    "            if preds[lb][i] == lbs[lb][i]:\n",
    "                upper = upper+1\n",
    "            ratio = upper/len(LABEL_COLUMNS)\n",
    "        sum_ratio = sum_ratio+ratio\n",
    "    return sum_ratio/len(preds)\n",
    "\n",
    "def get_labels(labels, record):\n",
    "  result = []\n",
    "  for i in range(len(record)):\n",
    "    if record[i] == 1:\n",
    "      result.append(labels[i])\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L8iJwETStxYc"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "LABEL_COLUMNS = train_df.columns.tolist()[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqiBwDNamMNh"
   },
   "source": [
    "# BOW - TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NZdP938yzruf"
   },
   "outputs": [],
   "source": [
    "def code_tokenizer(identifier):\n",
    "    #camelcase\n",
    "    identifier = re.sub(\"[^a-zA-Z]+\", \"\", identifier)\n",
    "    subtokens = set()\n",
    "    parts = filter(None, re.split(\"[, \\-!?:_~]+\", identifier))\n",
    "#     identifier.split('_')\n",
    "    for part in parts:\n",
    "        if not part.isdigit():\n",
    "            splitted = re.sub('([A-Z][a-z]+)', r' \\1', re.sub('([A-Z]+)', r' \\1', part)).split()\n",
    "            lower_tokens = [item.lower() for item in splitted]\n",
    "            subtokens.update(set(lower_tokens))\n",
    "    return subtokens\n",
    "\n",
    "def comment_remover(text):\n",
    "    def replacer(match):\n",
    "        s = match.group(0)\n",
    "        if s.startswith('/'):\n",
    "            return \" \"\n",
    "        else:\n",
    "            return s\n",
    "    pattern = re.compile(\n",
    "        r'//.*?$|/\\*.*?\\*/|\\'(?:\\\\.|[^\\\\\\'])*\\'|\"(?:\\\\.|[^\\\\\"])*\"',\n",
    "        re.DOTALL | re.MULTILINE\n",
    "    )\n",
    "    return re.sub(pattern, replacer, text)\n",
    "\n",
    "def string_lit_remover(text):\n",
    "    return re.sub(r'\".*\"', 'strlitplaceholder', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jdwtO_vN0WOs"
   },
   "outputs": [],
   "source": [
    "def code_tokenizer(identifier):\n",
    "    identifier = re.sub(\"[^a-zA-Z]+\", \"\", identifier)\n",
    "    result = \"\"\n",
    "    parts = filter(None, re.split(\"[, \\-!?:_~]+\", identifier))\n",
    "    for part in parts:\n",
    "        if not part.isdigit():\n",
    "            splitted = re.sub('([A-Z][a-z]+)', r' \\1', re.sub('([A-Z]+)', r' \\1', part)).split()\n",
    "            lower_tokens = [item.lower() for item in splitted]\n",
    "            for token in lower_tokens:\n",
    "              result = result+ \" \" + token\n",
    "    return result.strip()\n",
    "\n",
    "def merge_code(code):\n",
    "    output = \"\"\n",
    "    for unit in code:\n",
    "        output = output + \" \"+code_tokenizer(unit)\n",
    "    return output.strip()\n",
    "\n",
    "def preprocess_code(code_):\n",
    "  code_ = code_.splitlines()\n",
    "  result = \"\"\n",
    "  for line in code_:\n",
    "    line = re.sub('[^a-zA-Z0-9]', ' ', line)\n",
    "    line = re.sub(' +', ' ', line)\n",
    "    # line = re.sub(r\"([A-Z])\", r\" \\1\", line).split()\n",
    "    line = line.split(\" \")\n",
    "    line = merge_code(line)\n",
    "    result = result+\" \"+line\n",
    "  return result.strip()\n",
    "\n",
    "# print(preprocess_code(train_df['text'][5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upPHCA1clXwh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "khQH3PaO4dMH"
   },
   "outputs": [],
   "source": [
    "def preprocess_raw(df):\n",
    "    tag_col = []\n",
    "    for ind in df.index:\n",
    "        code = df['text'][ind]\n",
    "        code = comment_remover(code)\n",
    "        code = string_lit_remover(code)\n",
    "        df['text'][ind] = preprocess_code(code)\n",
    "        lbs = []\n",
    "        for lb in LABEL_COLUMNS:\n",
    "          if df[lb][ind] == 1:\n",
    "            lbs.append(lb)\n",
    "        tag_col.append(lbs)\n",
    "    df['tags']  = tag_col\n",
    "\n",
    "preprocess_raw(train_df)\n",
    "preprocess_raw(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EGxjkRgSmAME"
   },
   "outputs": [],
   "source": [
    "train_text = train_df['text']\n",
    "test_text = test_df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a6ZfhEs5mkH6",
    "outputId": "2b70cedc-bec7-46d4-8a26-d56f0a6db22b"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = feature_extraction.text.TfidfVectorizer(max_features=20000, ngram_range=(1,2))\n",
    "vectorizer.fit(train_text)\n",
    "vectorizer.fit(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mg6L--UCoZZl"
   },
   "outputs": [],
   "source": [
    "x_train = vectorizer.transform(train_text)\n",
    "y_train = train_df.drop(labels = ['index','text', 'tags'], axis=1)\n",
    "x_test = vectorizer.transform(test_text)\n",
    "y_test = test_df.drop(labels = ['index','text', 'tags', 'Unnamed: 0.1.1'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k96QBeRWoo6e"
   },
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cZBRiHmiorK6"
   },
   "outputs": [],
   "source": [
    "y_test = y_test[y_train.columns.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vLu5Itwwnuod"
   },
   "outputs": [],
   "source": [
    "!pip install scikit-multilearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g6BDZXZlu_7a"
   },
   "source": [
    "## BinaryRelevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "derwWUE6qAax"
   },
   "outputs": [],
   "source": [
    "# using binary relevance\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "# initialize binary relevance multi-label classifier\n",
    "# with a gaussian naive bayes base classifier\n",
    "classifier_bin = BinaryRelevance(GaussianNB())\n",
    "# train\n",
    "classifier_bin.fit(x_train, y_train)\n",
    "# predict\n",
    "# accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d-NZ61MToDiP",
    "outputId": "66ad0198-e5bd-4529-df77-dfbc84388d45"
   },
   "outputs": [],
   "source": [
    "# for row in predictions_bin:\n",
    "#   print(\"xx: \"+str(row.shape))\n",
    "predictions_bin = classifier_bin.predict(x_test)\n",
    "prediction_bin = pd.DataFrame(predictions_bin.toarray(), columns=LABEL_COLUMNS)\n",
    "print(\"Subset accuracy = \",accuracy_score(y_test, predictions_bin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wpp-LESqtM5T"
   },
   "outputs": [],
   "source": [
    "y_pred = prediction_bin.to_numpy()\n",
    "y_true = y_test.to_numpy()\n",
    "print(classification_report(\n",
    "  y_true, \n",
    "  y_pred, \n",
    "  target_names=LABEL_COLUMNS, \n",
    "  zero_division=0\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tzZmo4l8u5oI"
   },
   "outputs": [],
   "source": [
    "print(\"Hamming score: \" + str(hamming_score(prediction_bin, y_test)))\n",
    "print(\"Exact match ratio: \" + str(exact_match_ratio(prediction_bin, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NVBrnoeHsqnd"
   },
   "outputs": [],
   "source": [
    "prediction_bin.to_csv(\"base_results.csv\", sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T86y2pYNvCth"
   },
   "source": [
    "## ClassifierChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bpEKhksJqMDZ"
   },
   "outputs": [],
   "source": [
    "# using classifier chains\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# initialize classifier chains multi-label classifier\n",
    "classifier_chain = ClassifierChain(LogisticRegression())\n",
    "# Training logistic regression model on train data\n",
    "classifier_chain.fit(x_train, y_train)\n",
    "# predict\n",
    "predictions_chain = classifier_chain.predict(x_test)\n",
    "# accuracy\n",
    "print(\"Subset accuracy = \",accuracy_score(y_test,predictions_chain))\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ooARM-2fqbZZ"
   },
   "outputs": [],
   "source": [
    "predictions_chain = pd.DataFrame(predictions_chain.toarray(), columns=LABEL_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k_ILyhmRyUGr"
   },
   "outputs": [],
   "source": [
    "y_pred = predictions_chain.to_numpy()\n",
    "print(classification_report(\n",
    "  y_true, \n",
    "  y_pred, \n",
    "  target_names=LABEL_COLUMNS, \n",
    "  zero_division=0\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zvC4TxMSyX4P",
    "outputId": "1b065885-f122-4694-bf26-c6dcc529cb28"
   },
   "outputs": [],
   "source": [
    "print(\"Hamming score: \"+ str(hamming_score(predictions_chain, y_test)))\n",
    "print(\"Exact match ratio: \"+str(exact_match_ratio(predictions_chain, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D0PUWyVsydWv",
    "outputId": "c353b1b4-2fa4-4420-d213-dae1f54bc438"
   },
   "outputs": [],
   "source": [
    "## select observation\n",
    "i = 4\n",
    "txt_instance = test_df[\"text\"].iloc[i]\n",
    "## check true value and predicted value\n",
    "print(\"True:\", get_labels(LABEL_COLUMNS, y_true[i]), \"--> Pred:\", get_labels(LABEL_COLUMNS, y_pred[i]))\n",
    "## show explanation\n",
    "# explainer = lime_text.LimeTextExplainer(class_names=LABEL_COLUMNS)\n",
    "# explained = explainer.explain_instance(txt_instance, \n",
    "#              classifier.predict_proba, num_features=3)\n",
    "# explained.show_in_notebook(text=txt_instance, predict_proba=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VPlQHzcqzqr5"
   },
   "source": [
    "# Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eysBdoECokHu"
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0zuvttt7jhV5"
   },
   "source": [
    "## GLoVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bk3tjOoyqHb3"
   },
   "outputs": [],
   "source": [
    "corpus = train_df[\"text\"]\n",
    "\n",
    "## create list of lists of unigrams\n",
    "lst_corpus = []\n",
    "for string in corpus:\n",
    "   lst_words = string.split()\n",
    "   lst_corpus.append(lst_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X_OS_Y5zp5Ma"
   },
   "outputs": [],
   "source": [
    "## tokenize text\n",
    "tokenizer = kprocessing.text.Tokenizer(lower=True, split=' ', \n",
    "                     oov_token=\"NNaaNN\", \n",
    "                     filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts(lst_corpus)\n",
    "dic_vocabulary = tokenizer.word_index\n",
    "## create sequence\n",
    "lst_text2seq= tokenizer.texts_to_sequences(lst_corpus)\n",
    "## padding sequence\n",
    "X_train = kprocessing.sequence.pad_sequences(lst_text2seq, \n",
    "                    maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2yAuKdNwxZLE"
   },
   "outputs": [],
   "source": [
    "corpus_test = test_df[\"text\"]\n",
    "\n",
    "## create list of n-grams\n",
    "lst_corpus_test = []\n",
    "for string in corpus_test:\n",
    "    lst_words = string.split()\n",
    "    lst_corpus_test.append(lst_words)\n",
    "    \n",
    "# lst_corpus_test = list(trigrams_detector[lst_corpus_test])\n",
    "## text to sequence with the fitted tokenizer\n",
    "lst_text2seq = tokenizer.texts_to_sequences(lst_corpus_test)\n",
    "\n",
    "## padding sequence\n",
    "X_test = kprocessing.sequence.pad_sequences(lst_text2seq, maxlen=MAX_LEN,\n",
    "             padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DlDx72AfzpT4",
    "outputId": "6ab5bc3e-7f93-449f-9fb8-1ae2cc1c6769"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jDo5YhsNZmbi"
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import os\n",
    "\n",
    "gadget_glove_file_dir = 'YOUR_DIR/GloVe/gadget_vectors.txt'\n",
    "gadget_tmp_file_dir = 'YOUR_DIR/gadget_gensim.txt'\n",
    "gadget_temp_file = get_tmpfile(gadget_tmp_file_dir)\n",
    "if not os.path.isfile(gadget_tmp_file_dir):\n",
    "  glove2word2vec(gadget_glove_file_dir, gadget_temp_file)\n",
    "glove_embeddings = KeyedVectors.load_word2vec_format(gadget_temp_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NSqEGN3onCZv"
   },
   "outputs": [],
   "source": [
    "dic_vocabulary = {}\n",
    "vocab_file = open('YOUR_DIR/GloVe/vocab.txt', 'r')\n",
    "lines = vocab_file.readlines()\n",
    "for line in lines:\n",
    "  parts = line.split()\n",
    "  if len(parts) == 2:\n",
    "    dic_vocabulary[parts[0]] = int(parts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j0fVnCwYmYXZ",
    "outputId": "c4cb7869-6c29-43c5-c69d-46483b45d613"
   },
   "outputs": [],
   "source": [
    " vocab_size = glove_embeddings.wv.vectors.shape[0] \n",
    " embedding_size = glove_embeddings.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jfW2OJSnZmd3"
   },
   "outputs": [],
   "source": [
    "## start the matrix (length of vocabulary x vector size) with all 0s\n",
    "embeddings = np.zeros((len(dic_vocabulary)+1, embedding_size))\n",
    "for word,idx in dic_vocabulary.items():\n",
    "    ## update the row with vector\n",
    "    try:\n",
    "        embeddings[idx] =  glove_embeddings[word]\n",
    "    ## if word not in model then skip and the row stays all 0s\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8RzYHhrJZmgL",
    "outputId": "e8f72fd0-89bc-4824-97a3-8ddf55e0e903"
   },
   "outputs": [],
   "source": [
    "word = \"callback\"\n",
    "print(\"dic[word]:\", dic_vocabulary[word], \"|idx\")\n",
    "print(\"embeddings[idx]:\", embeddings[dic_vocabulary[word]].shape, \n",
    "      \"|vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XPvnHx2WoYQh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2uSZhzNXoYyG"
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1jWYw1d5oDjW"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "class Attention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(Attention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    " \n",
    "    def call(self, features, hidden):\n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = tf.nn.tanh(\n",
    "            self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "        \n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    " \n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eG-4V5GPoDlr",
    "outputId": "c6ea9fb8-73bd-4dcd-ea93-8e29809feba3"
   },
   "outputs": [],
   "source": [
    "# code attention layer\n",
    "def attention_layer(inputs, neurons):\n",
    "    x = layers.Permute((2,1))(inputs)\n",
    "    x = layers.Dense(neurons, activation=\"softmax\")(x)\n",
    "    x = layers.Permute((2,1), name=\"att\")(x)\n",
    "    x = layers.multiply([inputs, x])\n",
    "    return x\n",
    "\n",
    "## input\n",
    "x_in = layers.Input(shape=(MAX_LEN,))\n",
    "## embedding\n",
    "x = layers.Embedding(input_dim=embeddings.shape[0],  \n",
    "                     output_dim=embeddings.shape[1], \n",
    "                     weights=[embeddings],\n",
    "                     input_length=MAX_LEN, trainable=False)(x_in)\n",
    "## apply attention\n",
    "x = attention_layer(x, neurons=MAX_LEN)\n",
    "## 2 layers of bidirectional lstm\n",
    "x = layers.Bidirectional(layers.LSTM(units=MAX_LEN, dropout=0.2, return_sequences=True))(x)\n",
    "(lstm, forward_h, forward_c, backward_h, backward_c) = layers.Bidirectional(layers.LSTM(units=MAX_LEN, dropout=0.2, return_sequences=True, return_state=True))(x)\n",
    "state_h = Concatenate()([forward_h, backward_h])\n",
    "state_c = Concatenate()([forward_c, backward_c])\n",
    "\n",
    "context_vector, attention_weights = Attention(MAX_LEN)(lstm, state_h)\n",
    "## final dense layers\n",
    "x = layers.Dense(64, activation='relu')(context_vector)\n",
    "y_out = layers.Dense(len(LABEL_COLUMNS), activation='sigmoid')(x)\n",
    "## compile\n",
    "model = models.Model(x_in, y_out)\n",
    "# bp_mll_loss\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "# model.compile(loss='bp_mll_loss',\n",
    "#               optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1kWsam5voDoN"
   },
   "outputs": [],
   "source": [
    "y_train = train_df[LABEL_COLUMNS].values\n",
    "y_test = test_df[LABEL_COLUMNS].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7E9SclPKoDqi",
    "outputId": "612f8250-724e-4c4b-c7d5-59a15cfa5180"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "GLOVE_MODEL_PATH = \"YOUR_GLOVE_MODEL\"\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min')\n",
    "mcp_save = ModelCheckpoint(GLOVE_MODEL_PATH, save_best_only=True, monitor='val_loss', mode='min')\n",
    "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=1, epsilon=1e-4, mode='min')\n",
    "\n",
    "callbacks=[earlyStopping, mcp_save, reduce_lr_loss]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6jj8p7qEoDs-",
    "outputId": "3808aa53-91c5-48b5-d550-8db276c7c551"
   },
   "outputs": [],
   "source": [
    "training = model.fit(x=X_train, y=y_train, batch_size=128, \n",
    "                     epochs=50, shuffle=True, callbacks=callbacks, verbose=1,\n",
    "                     validation_data=(X_test, y_test))\n",
    "model.save_weights(GLOVE_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s-5q649loDvf",
    "outputId": "7fa9b418-0990-43e1-d95e-9ba0af9b996c"
   },
   "outputs": [],
   "source": [
    "predicted_prob = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wowkHgpmoDx6"
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "numpy.savetxt(\"YOUR_RESULT_DIR/results.csv\", predicted_prob, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XwhfR_hD6PWT"
   },
   "source": [
    "## Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iDO-jjyTzmoC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vffIFAvU6Swl"
   },
   "outputs": [],
   "source": [
    "corpus = train_df[\"text\"]\n",
    "\n",
    "## create list of lists of unigrams\n",
    "lst_corpus = []\n",
    "for string in corpus:\n",
    "   lst_words = string.split()\n",
    "   lst_grams = [\" \".join(lst_words[i:i+1]) \n",
    "               for i in range(0, len(lst_words), 1)]\n",
    "   lst_corpus.append(lst_grams)\n",
    "\n",
    "## detect bigrams and trigrams\n",
    "bigrams_detector = gensim.models.phrases.Phrases(lst_corpus, \n",
    "                 delimiter=\" \".encode(), min_count=5, threshold=10)\n",
    "bigrams_detector = gensim.models.phrases.Phraser(bigrams_detector)\n",
    "# trigrams_detector = gensim.models.phrases.Phrases(bigrams_detector[lst_corpus], \n",
    "            # delimiter=\" \".encode(), min_count=5, threshold=10)\n",
    "# trigrams_detector = gensim.models.phrases.Phraser(trigrams_detector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OtjKTF2MCkSS"
   },
   "outputs": [],
   "source": [
    "nlp = gensim.models.word2vec.Word2Vec(lst_corpus, size=300,   \n",
    "            window=25, min_count=1, sg=1, iter=30, workers=4)\n",
    "nlp.save(W2V_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zl9kpFPKChAa"
   },
   "outputs": [],
   "source": [
    "nlp = gensim.models.Word2Vec.load(W2V_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hcBIT8r58Wu1"
   },
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_9tgvVOBA3VG"
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X8KUTrSG8YOu"
   },
   "outputs": [],
   "source": [
    "## tokenize text\n",
    "tokenizer = kprocessing.text.Tokenizer(lower=True, split=' ', \n",
    "                     oov_token=\"NNaaNN\", \n",
    "                     filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts(lst_corpus)\n",
    "dic_vocabulary = tokenizer.word_index\n",
    "## create sequence\n",
    "lst_text2seq= tokenizer.texts_to_sequences(lst_corpus)\n",
    "## padding sequence\n",
    "X_train = kprocessing.sequence.pad_sequences(lst_text2seq, \n",
    "                    maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "Sn0H7uov-oEi",
    "outputId": "beaa46e6-2025-466a-d336-8f6362407f5e"
   },
   "outputs": [],
   "source": [
    "sns.heatmap(X_train==0, vmin=0, vmax=1, cbar=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PF83r7BF-q40"
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "## list of text: [\"I like this\", ...]\n",
    "len_txt = len(train_df[\"text\"].iloc[i].split())\n",
    "print(\"from: \", train_df[\"text\"].iloc[i], \"| len:\", len_txt)\n",
    "\n",
    "## sequence of token ids: [[1, 2, 3], ...]\n",
    "len_tokens = len(X_train[i])\n",
    "print(\"to: \", X_train[i], \"| len:\", len(X_train[i]))\n",
    "\n",
    "## vocabulary: {\"I\":1, \"like\":2, \"this\":3, ...}\n",
    "print(\"check: \", train_df[\"text\"].iloc[i].split()[0], \n",
    "      \" -- idx in vocabulary -->\", \n",
    "      dic_vocabulary[train_df[\"text\"].iloc[i].split()[0]])\n",
    "\n",
    "print(\"vocabulary: \", dict(list(dic_vocabulary.items())[0:10]), \"... (padding element, 0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n18mDYcSAOc4"
   },
   "outputs": [],
   "source": [
    "corpus_test = test_df[\"text\"]\n",
    "\n",
    "## create list of n-grams\n",
    "lst_corpus_test = []\n",
    "for string in corpus_test:\n",
    "    lst_words = string.split()\n",
    "    lst_grams = [\" \".join(lst_words[i:i+1]) for i in range(0, \n",
    "                 len(lst_words), 1)]\n",
    "    lst_corpus_test.append(lst_grams)\n",
    "    \n",
    "## detect common bigrams and trigrams using the fitted detectors\n",
    "lst_corpus_test = list(bigrams_detector[lst_corpus_test])\n",
    "# lst_corpus_test = list(trigrams_detector[lst_corpus_test])\n",
    "## text to sequence with the fitted tokenizer\n",
    "lst_text2seq = tokenizer.texts_to_sequences(lst_corpus_test)\n",
    "\n",
    "## padding sequence\n",
    "X_test = kprocessing.sequence.pad_sequences(lst_text2seq, maxlen=MAX_LEN,\n",
    "             padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SzyMoDxNBNnP"
   },
   "outputs": [],
   "source": [
    "## start the matrix (length of vocabulary x vector size) with all 0s\n",
    "embeddings = np.zeros((len(dic_vocabulary)+1, 300))\n",
    "for word,idx in dic_vocabulary.items():\n",
    "    ## update the row with vector\n",
    "    try:\n",
    "        embeddings[idx] =  nlp[word]\n",
    "    ## if word not in model then skip and the row stays all 0s\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MBQQTF18BZp4"
   },
   "outputs": [],
   "source": [
    "print(\"dic[word]:\", dic_vocabulary[word], \"|idx\")\n",
    "print(\"embeddings[idx]:\", embeddings[dic_vocabulary[word]].shape, \n",
    "      \"|vector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ei1X0wTCGqq"
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pOM-5f1EH2l1"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "class Attention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(Attention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    " \n",
    "    def call(self, features, hidden):\n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = tf.nn.tanh(\n",
    "            self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "        \n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    " \n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wB4n_U6oBhSO"
   },
   "outputs": [],
   "source": [
    "# code attention layer\n",
    "def attention_layer(inputs, neurons):\n",
    "    x = layers.Permute((2,1))(inputs)\n",
    "    x = layers.Dense(neurons, activation=\"softmax\")(x)\n",
    "    x = layers.Permute((2,1), name=\"att\")(x)\n",
    "    x = layers.multiply([inputs, x])\n",
    "    return x\n",
    "\n",
    "## input\n",
    "x_in = layers.Input(shape=(MAX_LEN,))\n",
    "## embedding\n",
    "x = layers.Embedding(input_dim=embeddings.shape[0],  \n",
    "                     output_dim=embeddings.shape[1], \n",
    "                     weights=[embeddings],\n",
    "                     input_length=MAX_LEN, trainable=False)(x_in)\n",
    "## apply attention\n",
    "x = attention_layer(x, neurons=MAX_LEN)\n",
    "## 2 layers of bidirectional lstm\n",
    "x = layers.Bidirectional(layers.LSTM(units=MAX_LEN, dropout=0.2, return_sequences=True))(x)\n",
    "(lstm, forward_h, forward_c, backward_h, backward_c) = layers.Bidirectional(layers.LSTM(units=MAX_LEN, dropout=0.2, return_sequences=True, return_state=True))(x)\n",
    "state_h = Concatenate()([forward_h, backward_h])\n",
    "state_c = Concatenate()([forward_c, backward_c])\n",
    "\n",
    "context_vector, attention_weights = Attention(MAX_LEN)(lstm, state_h)\n",
    "## final dense layers\n",
    "x = layers.Dense(64, activation='relu')(context_vector)\n",
    "y_out = layers.Dense(len(LABEL_COLUMNS), activation='sigmoid')(x)\n",
    "## compile\n",
    "model = models.Model(x_in, y_out)\n",
    "# bp_mll_loss\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "# model.compile(loss='bp_mll_loss',\n",
    "#               optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1CkjLn4uDBlb"
   },
   "outputs": [],
   "source": [
    "y_train = train_df[LABEL_COLUMNS].values\n",
    "y_test = test_df[LABEL_COLUMNS].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NHYsCZLUEeov"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K-HU0lmADxtc"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1o7cOriP1lVY",
    "outputId": "dce7c292-ef1e-49dc-974f-7c695d555573"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "W2V_MODEL_PATH = \"YOUR_SAVE_MODEL\"\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min')\n",
    "mcp_save = ModelCheckpoint(W2V_MODEL_PATH, save_best_only=True, monitor='val_loss', mode='min')\n",
    "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=1, epsilon=1e-4, mode='min')\n",
    "\n",
    "callbacks=[earlyStopping, mcp_save, reduce_lr_loss]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3a3fm6jcDzJJ"
   },
   "outputs": [],
   "source": [
    "## encode y\n",
    "# dic_y_mapping = {n:label for n,label in \n",
    "#                  enumerate(np.unique(y_train))}\n",
    "# inverse_dic = {v:k for k,v in dic_y_mapping.items()}\n",
    "# y_train = np.array([inverse_dic[y] for y in y_train])\n",
    "## train\n",
    "# training = model.fit(x=X_train, y=y_train, batch_size=256, \n",
    "#                      epochs=10, shuffle=True, verbose=0, \n",
    "#                      validation_split=0.3)\n",
    "training = model.fit(x=X_train, y=y_train, batch_size=128, \n",
    "                     epochs=50, shuffle=True, callbacks=callbacks, verbose=1,\n",
    "                     validation_data=(X_test, y_test))\n",
    "model.save_weights(W2V_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pZnt1xBV-FQr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uc3d0fvM9Ttz"
   },
   "outputs": [],
   "source": [
    "## plot loss and accuracy\n",
    "metrics = [k for k in training.history.keys() if (\"loss\" not in k) and (\"val\" not in k)]\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, sharey=True)\n",
    "ax[0].set(title=\"Training\")\n",
    "ax11 = ax[0].twinx()\n",
    "ax[0].plot(training.history['loss'], color='black')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Loss', color='black')\n",
    "for metric in metrics:\n",
    "    ax11.plot(training.history[metric], label=metric)\n",
    "ax11.set_ylabel(\"Score\", color='steelblue')\n",
    "ax11.legend()\n",
    "ax[1].set(title=\"Validation\")\n",
    "ax22 = ax[1].twinx()\n",
    "ax[1].plot(training.history['val_loss'], color='black')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Loss', color='black')\n",
    "for metric in metrics:\n",
    "     ax22.plot(training.history['val_'+metric], label=metric)\n",
    "ax22.set_ylabel(\"Score\", color=\"steelblue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rcBSNVSGFFEL"
   },
   "outputs": [],
   "source": [
    "# W2V_MODEL_PATH = \"SAVED_MODEL\"\n",
    "# model.save_weights(W2V_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nxsueQqhCVP2",
    "outputId": "b6914684-f337-4809-8309-71580dd5a2aa"
   },
   "outputs": [],
   "source": [
    "predicted_prob = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U6y-lu3aF09-"
   },
   "outputs": [],
   "source": [
    "THRESHOLD = YOUR_THRESHOLD\n",
    "upper, lower = 1, 0\n",
    "\n",
    "y_pred = np.where(predicted_prob > THRESHOLD, upper, lower)\n",
    "\n",
    "print(classification_report(\n",
    "  y_test, \n",
    "  y_pred, \n",
    "  target_names=LABEL_COLUMNS, \n",
    "  zero_division=0\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w-rzrrBRy4nO"
   },
   "outputs": [],
   "source": [
    "hamming_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I5imdPHJBRVY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "cqiBwDNamMNh"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
