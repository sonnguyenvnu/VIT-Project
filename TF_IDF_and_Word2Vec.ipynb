{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q16_onj8yBaL"
      },
      "outputs": [],
      "source": [
        "!pip install lime gensim keras transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vPrhopYxwTJ"
      },
      "outputs": [],
      "source": [
        "## for data\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "## for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "## for processing\n",
        "import re\n",
        "# import nltk\n",
        "## for bag-of-words\n",
        "from sklearn import feature_extraction, model_selection, naive_bayes, pipeline, manifold, preprocessing\n",
        "## for explainer\n",
        "from lime import lime_text\n",
        "## for word embedding\n",
        "import gensim\n",
        "import gensim.downloader as gensim_api\n",
        "## for deep learning\n",
        "from tensorflow.keras import models, layers, preprocessing as kprocessing\n",
        "from tensorflow.keras import backend as K\n",
        "## for bert language model\n",
        "import transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-processing"
      ],
      "metadata": {
        "id": "3hfASDNSjuqP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPtpRnYIx_JZ"
      },
      "outputs": [],
      "source": [
        "train_raw = pd.read_csv(\"YOUR_TRAIN_PATH\", sep='#')\n",
        "test_raw = pd.read_csv(\"YOUR_TEST_PATH\", sep='#')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GASE-UPCzmHn"
      },
      "outputs": [],
      "source": [
        "def preprocess(df):\n",
        "  removed_cols = ['Unnamed: 0',\n",
        "                  'Unnamed: 0.1',\n",
        " 'CVE Page',\n",
        " 'Summary',\n",
        " 'Vulnerability Classification',\n",
        " 'codeLink',\n",
        " 'commit_id',\n",
        " 'commit_message',\n",
        " 'del_lines',\n",
        " 'file_name',\n",
        " 'func_before',\n",
        " 'vul_func_with_fix',\n",
        " 'flaw_line',\n",
        " 'flaw_line_index',\n",
        " 'vul_func_with_fix',\n",
        " 'processed_func',\n",
        " 'sql',\n",
        " 'r.spl.',\n",
        " 'dir.',\n",
        " 'trav.',\n",
        " 'http',\n",
        " 'xss',\n",
        " 'corr.',\n",
        " 'cpg']\n",
        "  \n",
        "  try:\n",
        "      df = df.rename({'processed_func' : 'text', '+info': 'info', '+priv': 'priv', 'mem.' : 'mem'}, axis=1, inplace=False)\n",
        "  except:\n",
        "      print(\"RENAMED\")\n",
        "\n",
        "  for c in removed_cols:\n",
        "    try:\n",
        "      df = df.drop(c, axis=1)\n",
        "    except:\n",
        "      print(c)\n",
        "  df = df.drop('code', axis=1)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNcd02WQzogY"
      },
      "outputs": [],
      "source": [
        "train_df = preprocess(train_raw)\n",
        "test_df = preprocess(test_raw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8DlU18ttnjS"
      },
      "outputs": [],
      "source": [
        "def hamming_score(preds, lbs):\n",
        "    sum_ratio = 0.0\n",
        "    for i, row in preds.iterrows():\n",
        "        upper = 0\n",
        "        lower = 0\n",
        "        ratio = 0.0\n",
        "        for lb in LABEL_COLUMNS:\n",
        "            logic_and = preds[lb][i] + lbs[lb][i]\n",
        "            if logic_and == 2:\n",
        "                upper = upper+1\n",
        "                lower = lower+1\n",
        "            if logic_and == 1:\n",
        "                lower = lower+1\n",
        "        if lower != 0:\n",
        "            ratio = upper/lower\n",
        "        sum_ratio = sum_ratio+ratio\n",
        "    return sum_ratio/len(preds)\n",
        "\n",
        "def exact_match_ratio(preds, lbs):\n",
        "    sum_ratio = 0.0\n",
        "    for i, row in preds.iterrows():\n",
        "        upper = 0\n",
        "        ratio = 0.0\n",
        "        for lb in LABEL_COLUMNS:\n",
        "            if preds[lb][i] == lbs[lb][i]:\n",
        "                upper = upper+1\n",
        "            ratio = upper/len(LABEL_COLUMNS)\n",
        "        sum_ratio = sum_ratio+ratio\n",
        "    return sum_ratio/len(preds)\n",
        "\n",
        "def get_labels(labels, record):\n",
        "  result = []\n",
        "  for i in range(len(record)):\n",
        "    if record[i] == 1:\n",
        "      result.append(labels[i])\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8iJwETStxYc"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
        "LABEL_COLUMNS = train_df.columns.tolist()[2:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqiBwDNamMNh"
      },
      "source": [
        "# BOW - TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZdP938yzruf"
      },
      "outputs": [],
      "source": [
        "def code_tokenizer(identifier):\n",
        "    #camelcase\n",
        "    identifier = re.sub(\"[^a-zA-Z]+\", \"\", identifier)\n",
        "    subtokens = set()\n",
        "    parts = filter(None, re.split(\"[, \\-!?:_~]+\", identifier))\n",
        "#     identifier.split('_')\n",
        "    for part in parts:\n",
        "        if not part.isdigit():\n",
        "            splitted = re.sub('([A-Z][a-z]+)', r' \\1', re.sub('([A-Z]+)', r' \\1', part)).split()\n",
        "            lower_tokens = [item.lower() for item in splitted]\n",
        "            subtokens.update(set(lower_tokens))\n",
        "    return subtokens\n",
        "\n",
        "def comment_remover(text):\n",
        "    def replacer(match):\n",
        "        s = match.group(0)\n",
        "        if s.startswith('/'):\n",
        "            return \" \"\n",
        "        else:\n",
        "            return s\n",
        "    pattern = re.compile(\n",
        "        r'//.*?$|/\\*.*?\\*/|\\'(?:\\\\.|[^\\\\\\'])*\\'|\"(?:\\\\.|[^\\\\\"])*\"',\n",
        "        re.DOTALL | re.MULTILINE\n",
        "    )\n",
        "    return re.sub(pattern, replacer, text)\n",
        "\n",
        "def string_lit_remover(text):\n",
        "    return re.sub(r'\".*\"', 'strlitplaceholder', text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdwtO_vN0WOs"
      },
      "outputs": [],
      "source": [
        "def code_tokenizer(identifier):\n",
        "    identifier = re.sub(\"[^a-zA-Z]+\", \"\", identifier)\n",
        "    result = \"\"\n",
        "    parts = filter(None, re.split(\"[, \\-!?:_~]+\", identifier))\n",
        "    for part in parts:\n",
        "        if not part.isdigit():\n",
        "            splitted = re.sub('([A-Z][a-z]+)', r' \\1', re.sub('([A-Z]+)', r' \\1', part)).split()\n",
        "            lower_tokens = [item.lower() for item in splitted]\n",
        "            for token in lower_tokens:\n",
        "              result = result+ \" \" + token\n",
        "    return result.strip()\n",
        "\n",
        "def merge_code(code):\n",
        "    output = \"\"\n",
        "    for unit in code:\n",
        "        output = output + \" \"+code_tokenizer(unit)\n",
        "    return output.strip()\n",
        "\n",
        "def preprocess_code(code_):\n",
        "  code_ = code_.splitlines()\n",
        "  result = \"\"\n",
        "  for line in code_:\n",
        "    line = re.sub('[^a-zA-Z0-9]', ' ', line)\n",
        "    line = re.sub(' +', ' ', line)\n",
        "    # line = re.sub(r\"([A-Z])\", r\" \\1\", line).split()\n",
        "    line = line.split(\" \")\n",
        "    line = merge_code(line)\n",
        "    result = result+\" \"+line\n",
        "  return result.strip()\n",
        "\n",
        "# print(preprocess_code(train_df['text'][5]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upPHCA1clXwh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khQH3PaO4dMH"
      },
      "outputs": [],
      "source": [
        "def preprocess_raw(df):\n",
        "    tag_col = []\n",
        "    for ind in df.index:\n",
        "        code = df['text'][ind]\n",
        "        code = comment_remover(code)\n",
        "        code = string_lit_remover(code)\n",
        "        df['text'][ind] = preprocess_code(code)\n",
        "        lbs = []\n",
        "        for lb in LABEL_COLUMNS:\n",
        "          if df[lb][ind] == 1:\n",
        "            lbs.append(lb)\n",
        "        tag_col.append(lbs)\n",
        "    df['tags']  = tag_col\n",
        "\n",
        "preprocess_raw(train_df)\n",
        "preprocess_raw(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGxjkRgSmAME"
      },
      "outputs": [],
      "source": [
        "train_text = train_df['text']\n",
        "test_text = test_df['text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6ZfhEs5mkH6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b70cedc-bec7-46d4-8a26-d56f0a6db22b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfVectorizer(max_features=20000, ngram_range=(1, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = feature_extraction.text.TfidfVectorizer(max_features=20000, ngram_range=(1,2))\n",
        "vectorizer.fit(train_text)\n",
        "vectorizer.fit(test_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mg6L--UCoZZl"
      },
      "outputs": [],
      "source": [
        "x_train = vectorizer.transform(train_text)\n",
        "y_train = train_df.drop(labels = ['index','text', 'tags'], axis=1)\n",
        "x_test = vectorizer.transform(test_text)\n",
        "y_test = test_df.drop(labels = ['index','text', 'tags', 'Unnamed: 0.1.1'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k96QBeRWoo6e"
      },
      "outputs": [],
      "source": [
        "y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZBRiHmiorK6"
      },
      "outputs": [],
      "source": [
        "y_test = y_test[y_train.columns.tolist()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLu5Itwwnuod"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-multilearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6BDZXZlu_7a"
      },
      "source": [
        "## BinaryRelevance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "derwWUE6qAax"
      },
      "outputs": [],
      "source": [
        "# using binary relevance\n",
        "from skmultilearn.problem_transform import BinaryRelevance\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "# initialize binary relevance multi-label classifier\n",
        "# with a gaussian naive bayes base classifier\n",
        "classifier_bin = BinaryRelevance(GaussianNB())\n",
        "# train\n",
        "classifier_bin.fit(x_train, y_train)\n",
        "# predict\n",
        "# accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-NZ61MToDiP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66ad0198-e5bd-4529-df77-dfbc84388d45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subset accuracy =  0.5383886255924171\n"
          ]
        }
      ],
      "source": [
        "# for row in predictions_bin:\n",
        "#   print(\"xx: \"+str(row.shape))\n",
        "predictions_bin = classifier_bin.predict(x_test)\n",
        "prediction_bin = pd.DataFrame(predictions_bin.toarray(), columns=LABEL_COLUMNS)\n",
        "print(\"Subset accuracy = \",accuracy_score(y_test, predictions_bin))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wpp-LESqtM5T"
      },
      "outputs": [],
      "source": [
        "y_pred = prediction_bin.to_numpy()\n",
        "y_true = y_test.to_numpy()\n",
        "print(classification_report(\n",
        "  y_true, \n",
        "  y_pred, \n",
        "  target_names=LABEL_COLUMNS, \n",
        "  zero_division=0\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzZmo4l8u5oI"
      },
      "outputs": [],
      "source": [
        "print(\"Hamming score: \" + str(hamming_score(prediction_bin, y_test)))\n",
        "print(\"Exact match ratio: \" + str(exact_match_ratio(prediction_bin, y_test)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_bin.to_csv(\"base_results.csv\", sep=',', index=False)"
      ],
      "metadata": {
        "id": "NVBrnoeHsqnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T86y2pYNvCth"
      },
      "source": [
        "## ClassifierChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpEKhksJqMDZ"
      },
      "outputs": [],
      "source": [
        "# using classifier chains\n",
        "from skmultilearn.problem_transform import ClassifierChain\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# initialize classifier chains multi-label classifier\n",
        "classifier_chain = ClassifierChain(LogisticRegression())\n",
        "# Training logistic regression model on train data\n",
        "classifier_chain.fit(x_train, y_train)\n",
        "# predict\n",
        "predictions_chain = classifier_chain.predict(x_test)\n",
        "# accuracy\n",
        "print(\"Subset accuracy = \",accuracy_score(y_test,predictions_chain))\n",
        "print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooARM-2fqbZZ"
      },
      "outputs": [],
      "source": [
        "predictions_chain = pd.DataFrame(predictions_chain.toarray(), columns=LABEL_COLUMNS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_ILyhmRyUGr"
      },
      "outputs": [],
      "source": [
        "y_pred = predictions_chain.to_numpy()\n",
        "print(classification_report(\n",
        "  y_true, \n",
        "  y_pred, \n",
        "  target_names=LABEL_COLUMNS, \n",
        "  zero_division=0\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvC4TxMSyX4P",
        "outputId": "1b065885-f122-4694-bf26-c6dcc529cb28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hamming score: 0.6018957345971564\n",
            "Exact match ratio: 0.8850710900473934\n"
          ]
        }
      ],
      "source": [
        "print(\"Hamming score: \"+ str(hamming_score(predictions_chain, y_test)))\n",
        "print(\"Exact match ratio: \"+str(exact_match_ratio(predictions_chain, y_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0PUWyVsydWv",
        "outputId": "c353b1b4-2fa4-4420-d213-dae1f54bc438"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True: ['dos'] --> Pred: ['dos']\n"
          ]
        }
      ],
      "source": [
        "## select observation\n",
        "i = 4\n",
        "txt_instance = test_df[\"text\"].iloc[i]\n",
        "## check true value and predicted value\n",
        "print(\"True:\", get_labels(LABEL_COLUMNS, y_true[i]), \"--> Pred:\", get_labels(LABEL_COLUMNS, y_pred[i]))\n",
        "## show explanation\n",
        "# explainer = lime_text.LimeTextExplainer(class_names=LABEL_COLUMNS)\n",
        "# explained = explainer.explain_instance(txt_instance, \n",
        "#              classifier.predict_proba, num_features=3)\n",
        "# explained.show_in_notebook(text=txt_instance, predict_proba=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPlQHzcqzqr5"
      },
      "source": [
        "# Neural Net"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 512"
      ],
      "metadata": {
        "id": "eysBdoECokHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GLoVe"
      ],
      "metadata": {
        "id": "0zuvttt7jhV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = train_df[\"text\"]\n",
        "\n",
        "## create list of lists of unigrams\n",
        "lst_corpus = []\n",
        "for string in corpus:\n",
        "   lst_words = string.split()\n",
        "   lst_corpus.append(lst_words)"
      ],
      "metadata": {
        "id": "bk3tjOoyqHb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## tokenize text\n",
        "tokenizer = kprocessing.text.Tokenizer(lower=True, split=' ', \n",
        "                     oov_token=\"NNaaNN\", \n",
        "                     filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
        "tokenizer.fit_on_texts(lst_corpus)\n",
        "dic_vocabulary = tokenizer.word_index\n",
        "## create sequence\n",
        "lst_text2seq= tokenizer.texts_to_sequences(lst_corpus)\n",
        "## padding sequence\n",
        "X_train = kprocessing.sequence.pad_sequences(lst_text2seq, \n",
        "                    maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")"
      ],
      "metadata": {
        "id": "X_OS_Y5zp5Ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_test = test_df[\"text\"]\n",
        "\n",
        "## create list of n-grams\n",
        "lst_corpus_test = []\n",
        "for string in corpus_test:\n",
        "    lst_words = string.split()\n",
        "    lst_corpus_test.append(lst_words)\n",
        "    \n",
        "# lst_corpus_test = list(trigrams_detector[lst_corpus_test])\n",
        "## text to sequence with the fitted tokenizer\n",
        "lst_text2seq = tokenizer.texts_to_sequences(lst_corpus_test)\n",
        "\n",
        "## padding sequence\n",
        "X_test = kprocessing.sequence.pad_sequences(lst_text2seq, maxlen=MAX_LEN,\n",
        "             padding=\"post\", truncating=\"post\")"
      ],
      "metadata": {
        "id": "2yAuKdNwxZLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlDx72AfzpT4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ab5bc3e-7f93-449f-9fb8-1ae2cc1c6769"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[    53,     42, 104237, ...,      0,      0,      0],\n",
              "       [    42, 104293,  53725, ...,      0,      0,      0],\n",
              "       [104301, 104302,      3, ...,      0,      0,      0],\n",
              "       ...,\n",
              "       [    42, 274478, 274479, ...,      0,      0,      0],\n",
              "       [    42, 274483,  85102, ...,      0,      0,      0],\n",
              "       [    53,    117, 274507, ...,   3388,      6,     17]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.test.utils import get_tmpfile\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "import os\n",
        "\n",
        "gadget_glove_file_dir = '/content/drive/MyDrive/Son/vul-type/GloVe/gadget_vectors.txt'\n",
        "gadget_tmp_file_dir = '/content/drive/MyDrive/Son/vul-type/GloVe/gadget_gensim.txt'\n",
        "gadget_temp_file = get_tmpfile(gadget_tmp_file_dir)\n",
        "if not os.path.isfile(gadget_tmp_file_dir):\n",
        "  glove2word2vec(gadget_glove_file_dir, gadget_temp_file)\n",
        "glove_embeddings = KeyedVectors.load_word2vec_format(gadget_temp_file) "
      ],
      "metadata": {
        "id": "jDo5YhsNZmbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dic_vocabulary = {}\n",
        "vocab_file = open('/content/drive/MyDrive/Son/vul-type/GloVe/vocab.txt', 'r')\n",
        "lines = vocab_file.readlines()\n",
        "for line in lines:\n",
        "  parts = line.split()\n",
        "  if len(parts) == 2:\n",
        "    dic_vocabulary[parts[0]] = int(parts[1])"
      ],
      "metadata": {
        "id": "NSqEGN3onCZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " vocab_size = glove_embeddings.wv.vectors.shape[0] \n",
        " embedding_size = glove_embeddings.vector_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0fVnCwYmYXZ",
        "outputId": "c4cb7869-6c29-43c5-c69d-46483b45d613"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## start the matrix (length of vocabulary x vector size) with all 0s\n",
        "embeddings = np.zeros((len(dic_vocabulary)+1, embedding_size))\n",
        "for word,idx in dic_vocabulary.items():\n",
        "    ## update the row with vector\n",
        "    try:\n",
        "        embeddings[idx] =  glove_embeddings[word]\n",
        "    ## if word not in model then skip and the row stays all 0s\n",
        "    except:\n",
        "        pass"
      ],
      "metadata": {
        "id": "jfW2OJSnZmd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word = \"callback\"\n",
        "print(\"dic[word]:\", dic_vocabulary[word], \"|idx\")\n",
        "print(\"embeddings[idx]:\", embeddings[dic_vocabulary[word]].shape, \n",
        "      \"|vector\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RzYHhrJZmgL",
        "outputId": "e8f72fd0-89bc-4824-97a3-8ddf55e0e903"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dic[word]: 20 |idx\n",
            "embeddings[idx]: (256,) |vector\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XPvnHx2WoYQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "2uSZhzNXoYyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Concatenate\n",
        "class Attention(tf.keras.Model):\n",
        "    def __init__(self, units):\n",
        "        super(Attention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        " \n",
        "    def call(self, features, hidden):\n",
        "        # hidden shape == (batch_size, hidden size)\n",
        "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "        # we are doing this to perform addition to calculate the score\n",
        "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "\n",
        "        # score shape == (batch_size, max_length, 1)\n",
        "        # we get 1 at the last axis because we are applying score to self.V\n",
        "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "        score = tf.nn.tanh(\n",
        "            self.W1(features) + self.W2(hidden_with_time_axis))\n",
        "        \n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
        "\n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * features\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        " \n",
        "        return context_vector, attention_weights"
      ],
      "metadata": {
        "id": "1jWYw1d5oDjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# code attention layer\n",
        "def attention_layer(inputs, neurons):\n",
        "    x = layers.Permute((2,1))(inputs)\n",
        "    x = layers.Dense(neurons, activation=\"softmax\")(x)\n",
        "    x = layers.Permute((2,1), name=\"att\")(x)\n",
        "    x = layers.multiply([inputs, x])\n",
        "    return x\n",
        "\n",
        "## input\n",
        "x_in = layers.Input(shape=(MAX_LEN,))\n",
        "## embedding\n",
        "x = layers.Embedding(input_dim=embeddings.shape[0],  \n",
        "                     output_dim=embeddings.shape[1], \n",
        "                     weights=[embeddings],\n",
        "                     input_length=MAX_LEN, trainable=False)(x_in)\n",
        "## apply attention\n",
        "x = attention_layer(x, neurons=MAX_LEN)\n",
        "## 2 layers of bidirectional lstm\n",
        "x = layers.Bidirectional(layers.LSTM(units=MAX_LEN, dropout=0.2, return_sequences=True))(x)\n",
        "(lstm, forward_h, forward_c, backward_h, backward_c) = layers.Bidirectional(layers.LSTM(units=MAX_LEN, dropout=0.2, return_sequences=True, return_state=True))(x)\n",
        "state_h = Concatenate()([forward_h, backward_h])\n",
        "state_c = Concatenate()([forward_c, backward_c])\n",
        "\n",
        "context_vector, attention_weights = Attention(MAX_LEN)(lstm, state_h)\n",
        "## final dense layers\n",
        "x = layers.Dense(64, activation='relu')(context_vector)\n",
        "y_out = layers.Dense(len(LABEL_COLUMNS), activation='sigmoid')(x)\n",
        "## compile\n",
        "model = models.Model(x_in, y_out)\n",
        "# bp_mll_loss\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam', metrics=['accuracy'])\n",
        "# model.compile(loss='bp_mll_loss',\n",
        "#               optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eG-4V5GPoDlr",
        "outputId": "c6ea9fb8-73bd-4dcd-ea93-8e29809feba3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 512, 256)     70711296    ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " permute (Permute)              (None, 256, 512)     0           ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 256, 512)     262656      ['permute[0][0]']                \n",
            "                                                                                                  \n",
            " att (Permute)                  (None, 512, 256)     0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " multiply (Multiply)            (None, 512, 256)     0           ['embedding[0][0]',              \n",
            "                                                                  'att[0][0]']                    \n",
            "                                                                                                  \n",
            " bidirectional (Bidirectional)  (None, 512, 1024)    3149824     ['multiply[0][0]']               \n",
            "                                                                                                  \n",
            " bidirectional_1 (Bidirectional  [(None, 512, 1024),  6295552    ['bidirectional[0][0]']          \n",
            " )                               (None, 512),                                                     \n",
            "                                 (None, 512),                                                     \n",
            "                                 (None, 512),                                                     \n",
            "                                 (None, 512)]                                                     \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 1024)         0           ['bidirectional_1[0][1]',        \n",
            "                                                                  'bidirectional_1[0][3]']        \n",
            "                                                                                                  \n",
            " attention (Attention)          ((None, 1024),       1050113     ['bidirectional_1[0][0]',        \n",
            "                                 (None, 512, 1))                  'concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 64)           65600       ['attention[0][0]']              \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 8)            520         ['dense_4[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 81,535,561\n",
            "Trainable params: 10,824,265\n",
            "Non-trainable params: 70,711,296\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = train_df[LABEL_COLUMNS].values\n",
        "y_test = test_df[LABEL_COLUMNS].values"
      ],
      "metadata": {
        "id": "1kWsam5voDoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "GLOVE_MODEL_PATH = \"/content/drive/MyDrive/Son/vul-type/GloVe/glove_model_att.h5\"\n",
        "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min')\n",
        "mcp_save = ModelCheckpoint(GLOVE_MODEL_PATH, save_best_only=True, monitor='val_loss', mode='min')\n",
        "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=1, epsilon=1e-4, mode='min')\n",
        "\n",
        "callbacks=[earlyStopping, mcp_save, reduce_lr_loss]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7E9SclPKoDqi",
        "outputId": "612f8250-724e-4c4b-c7d5-59a15cfa5180"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training = model.fit(x=X_train, y=y_train, batch_size=128, \n",
        "                     epochs=50, shuffle=True, callbacks=callbacks, verbose=1,\n",
        "                     validation_data=(X_test, y_test))\n",
        "model.save_weights(GLOVE_MODEL_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jj8p7qEoDs-",
        "outputId": "3808aa53-91c5-48b5-d550-8db276c7c551"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "69/69 [==============================] - 104s 1s/step - loss: 0.4234 - accuracy: 0.4433 - val_loss: 0.4055 - val_accuracy: 0.4749 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "69/69 [==============================] - 89s 1s/step - loss: 0.4019 - accuracy: 0.4575 - val_loss: 0.4063 - val_accuracy: 0.4749 - lr: 0.0010\n",
            "Epoch 3/50\n",
            "69/69 [==============================] - ETA: 0s - loss: 0.4009 - accuracy: 0.4575\n",
            "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "69/69 [==============================] - 89s 1s/step - loss: 0.4009 - accuracy: 0.4575 - val_loss: 0.4061 - val_accuracy: 0.4749 - lr: 0.0010\n",
            "Epoch 4/50\n",
            "69/69 [==============================] - 94s 1s/step - loss: 0.4000 - accuracy: 0.4575 - val_loss: 0.4054 - val_accuracy: 0.4749 - lr: 1.0000e-04\n",
            "Epoch 5/50\n",
            "69/69 [==============================] - ETA: 0s - loss: 0.4000 - accuracy: 0.4575\n",
            "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "69/69 [==============================] - 89s 1s/step - loss: 0.4000 - accuracy: 0.4575 - val_loss: 0.4054 - val_accuracy: 0.4749 - lr: 1.0000e-04\n",
            "Epoch 6/50\n",
            "69/69 [==============================] - 88s 1s/step - loss: 0.3998 - accuracy: 0.4575 - val_loss: 0.4055 - val_accuracy: 0.4749 - lr: 1.0000e-05\n",
            "Epoch 7/50\n",
            "69/69 [==============================] - ETA: 0s - loss: 0.3998 - accuracy: 0.4575\n",
            "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "69/69 [==============================] - 89s 1s/step - loss: 0.3998 - accuracy: 0.4575 - val_loss: 0.4055 - val_accuracy: 0.4749 - lr: 1.0000e-05\n",
            "Epoch 8/50\n",
            "69/69 [==============================] - 89s 1s/step - loss: 0.3998 - accuracy: 0.4575 - val_loss: 0.4055 - val_accuracy: 0.4749 - lr: 1.0000e-06\n",
            "Epoch 9/50\n",
            "69/69 [==============================] - ETA: 0s - loss: 0.3998 - accuracy: 0.4575\n",
            "Epoch 9: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
            "69/69 [==============================] - 89s 1s/step - loss: 0.3998 - accuracy: 0.4575 - val_loss: 0.4055 - val_accuracy: 0.4749 - lr: 1.0000e-06\n",
            "Epoch 10/50\n",
            "69/69 [==============================] - 88s 1s/step - loss: 0.3998 - accuracy: 0.4575 - val_loss: 0.4055 - val_accuracy: 0.4749 - lr: 1.0000e-07\n",
            "Epoch 11/50\n",
            "69/69 [==============================] - ETA: 0s - loss: 0.3998 - accuracy: 0.4575\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
            "69/69 [==============================] - 89s 1s/step - loss: 0.3998 - accuracy: 0.4575 - val_loss: 0.4055 - val_accuracy: 0.4749 - lr: 1.0000e-07\n",
            "Epoch 12/50\n",
            "69/69 [==============================] - 89s 1s/step - loss: 0.3998 - accuracy: 0.4575 - val_loss: 0.4055 - val_accuracy: 0.4749 - lr: 1.0000e-08\n",
            "Epoch 13/50\n",
            "69/69 [==============================] - ETA: 0s - loss: 0.3998 - accuracy: 0.4575\n",
            "Epoch 13: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
            "69/69 [==============================] - 88s 1s/step - loss: 0.3998 - accuracy: 0.4575 - val_loss: 0.4055 - val_accuracy: 0.4749 - lr: 1.0000e-08\n",
            "Epoch 14/50\n",
            "69/69 [==============================] - 88s 1s/step - loss: 0.3998 - accuracy: 0.4575 - val_loss: 0.4055 - val_accuracy: 0.4749 - lr: 1.0000e-09\n",
            "Epoch 14: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_prob = model.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-5q649loDvf",
        "outputId": "7fa9b418-0990-43e1-d95e-9ba0af9b996c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33/33 [==============================] - 6s 143ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "numpy.savetxt(\"/content/drive/MyDrive/Son/vul-type/GloVe/results.csv\", predicted_prob, delimiter=\",\")"
      ],
      "metadata": {
        "id": "wowkHgpmoDx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7rZq6BIToD0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YxqklKq8oD23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TZcgnSxKoD5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "45kbWYhoZmiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hb-Pgj1RZmk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwhfR_hD6PWT"
      },
      "source": [
        "## Word2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDO-jjyTzmoC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vffIFAvU6Swl"
      },
      "outputs": [],
      "source": [
        "corpus = train_df[\"text\"]\n",
        "\n",
        "## create list of lists of unigrams\n",
        "lst_corpus = []\n",
        "for string in corpus:\n",
        "   lst_words = string.split()\n",
        "   lst_grams = [\" \".join(lst_words[i:i+1]) \n",
        "               for i in range(0, len(lst_words), 1)]\n",
        "   lst_corpus.append(lst_grams)\n",
        "\n",
        "## detect bigrams and trigrams\n",
        "bigrams_detector = gensim.models.phrases.Phrases(lst_corpus, \n",
        "                 delimiter=\" \".encode(), min_count=5, threshold=10)\n",
        "bigrams_detector = gensim.models.phrases.Phraser(bigrams_detector)\n",
        "# trigrams_detector = gensim.models.phrases.Phrases(bigrams_detector[lst_corpus], \n",
        "            # delimiter=\" \".encode(), min_count=5, threshold=10)\n",
        "# trigrams_detector = gensim.models.phrases.Phraser(trigrams_detector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ef2Xt3q67NrR"
      },
      "outputs": [],
      "source": [
        "## Saved model\n",
        "# W2V_PATH = \"/content/drive/MyDrive/Son/vul-type/w2v/w2v_window_50.model\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = gensim.models.word2vec.Word2Vec(lst_corpus, size=300,   \n",
        "            window=25, min_count=1, sg=1, iter=30, workers=4)\n",
        "nlp.save(W2V_PATH)"
      ],
      "metadata": {
        "id": "OtjKTF2MCkSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = gensim.models.Word2Vec.load(W2V_PATH)"
      ],
      "metadata": {
        "id": "zl9kpFPKChAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcBIT8r58Wu1"
      },
      "source": [
        "### Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9tgvVOBA3VG"
      },
      "outputs": [],
      "source": [
        "MAX_LEN = 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8KUTrSG8YOu"
      },
      "outputs": [],
      "source": [
        "## tokenize text\n",
        "tokenizer = kprocessing.text.Tokenizer(lower=True, split=' ', \n",
        "                     oov_token=\"NNaaNN\", \n",
        "                     filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
        "tokenizer.fit_on_texts(lst_corpus)\n",
        "dic_vocabulary = tokenizer.word_index\n",
        "## create sequence\n",
        "lst_text2seq= tokenizer.texts_to_sequences(lst_corpus)\n",
        "## padding sequence\n",
        "X_train = kprocessing.sequence.pad_sequences(lst_text2seq, \n",
        "                    maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "Sn0H7uov-oEi",
        "outputId": "beaa46e6-2025-466a-d336-8f6362407f5e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "sns.heatmap(X_train==0, vmin=0, vmax=1, cbar=False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PF83r7BF-q40"
      },
      "outputs": [],
      "source": [
        "i = 0\n",
        "\n",
        "## list of text: [\"I like this\", ...]\n",
        "len_txt = len(train_df[\"text\"].iloc[i].split())\n",
        "print(\"from: \", train_df[\"text\"].iloc[i], \"| len:\", len_txt)\n",
        "\n",
        "## sequence of token ids: [[1, 2, 3], ...]\n",
        "len_tokens = len(X_train[i])\n",
        "print(\"to: \", X_train[i], \"| len:\", len(X_train[i]))\n",
        "\n",
        "## vocabulary: {\"I\":1, \"like\":2, \"this\":3, ...}\n",
        "print(\"check: \", train_df[\"text\"].iloc[i].split()[0], \n",
        "      \" -- idx in vocabulary -->\", \n",
        "      dic_vocabulary[train_df[\"text\"].iloc[i].split()[0]])\n",
        "\n",
        "print(\"vocabulary: \", dict(list(dic_vocabulary.items())[0:10]), \"... (padding element, 0)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n18mDYcSAOc4"
      },
      "outputs": [],
      "source": [
        "corpus_test = test_df[\"text\"]\n",
        "\n",
        "## create list of n-grams\n",
        "lst_corpus_test = []\n",
        "for string in corpus_test:\n",
        "    lst_words = string.split()\n",
        "    lst_grams = [\" \".join(lst_words[i:i+1]) for i in range(0, \n",
        "                 len(lst_words), 1)]\n",
        "    lst_corpus_test.append(lst_grams)\n",
        "    \n",
        "## detect common bigrams and trigrams using the fitted detectors\n",
        "lst_corpus_test = list(bigrams_detector[lst_corpus_test])\n",
        "# lst_corpus_test = list(trigrams_detector[lst_corpus_test])\n",
        "## text to sequence with the fitted tokenizer\n",
        "lst_text2seq = tokenizer.texts_to_sequences(lst_corpus_test)\n",
        "\n",
        "## padding sequence\n",
        "X_test = kprocessing.sequence.pad_sequences(lst_text2seq, maxlen=MAX_LEN,\n",
        "             padding=\"post\", truncating=\"post\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzyMoDxNBNnP"
      },
      "outputs": [],
      "source": [
        "## start the matrix (length of vocabulary x vector size) with all 0s\n",
        "embeddings = np.zeros((len(dic_vocabulary)+1, 300))\n",
        "for word,idx in dic_vocabulary.items():\n",
        "    ## update the row with vector\n",
        "    try:\n",
        "        embeddings[idx] =  nlp[word]\n",
        "    ## if word not in model then skip and the row stays all 0s\n",
        "    except:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBQQTF18BZp4"
      },
      "outputs": [],
      "source": [
        "print(\"dic[word]:\", dic_vocabulary[word], \"|idx\")\n",
        "print(\"embeddings[idx]:\", embeddings[dic_vocabulary[word]].shape, \n",
        "      \"|vector\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ei1X0wTCGqq"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Concatenate\n",
        "class Attention(tf.keras.Model):\n",
        "    def __init__(self, units):\n",
        "        super(Attention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        " \n",
        "    def call(self, features, hidden):\n",
        "        # hidden shape == (batch_size, hidden size)\n",
        "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "        # we are doing this to perform addition to calculate the score\n",
        "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "\n",
        "        # score shape == (batch_size, max_length, 1)\n",
        "        # we get 1 at the last axis because we are applying score to self.V\n",
        "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "        score = tf.nn.tanh(\n",
        "            self.W1(features) + self.W2(hidden_with_time_axis))\n",
        "        \n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
        "\n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * features\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        " \n",
        "        return context_vector, attention_weights"
      ],
      "metadata": {
        "id": "pOM-5f1EH2l1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wB4n_U6oBhSO"
      },
      "outputs": [],
      "source": [
        "# code attention layer\n",
        "def attention_layer(inputs, neurons):\n",
        "    x = layers.Permute((2,1))(inputs)\n",
        "    x = layers.Dense(neurons, activation=\"softmax\")(x)\n",
        "    x = layers.Permute((2,1), name=\"att\")(x)\n",
        "    x = layers.multiply([inputs, x])\n",
        "    return x\n",
        "\n",
        "## input\n",
        "x_in = layers.Input(shape=(MAX_LEN,))\n",
        "## embedding\n",
        "x = layers.Embedding(input_dim=embeddings.shape[0],  \n",
        "                     output_dim=embeddings.shape[1], \n",
        "                     weights=[embeddings],\n",
        "                     input_length=MAX_LEN, trainable=False)(x_in)\n",
        "## apply attention\n",
        "x = attention_layer(x, neurons=MAX_LEN)\n",
        "## 2 layers of bidirectional lstm\n",
        "x = layers.Bidirectional(layers.LSTM(units=MAX_LEN, dropout=0.2, return_sequences=True))(x)\n",
        "(lstm, forward_h, forward_c, backward_h, backward_c) = layers.Bidirectional(layers.LSTM(units=MAX_LEN, dropout=0.2, return_sequences=True, return_state=True))(x)\n",
        "state_h = Concatenate()([forward_h, backward_h])\n",
        "state_c = Concatenate()([forward_c, backward_c])\n",
        "\n",
        "context_vector, attention_weights = Attention(MAX_LEN)(lstm, state_h)\n",
        "## final dense layers\n",
        "x = layers.Dense(64, activation='relu')(context_vector)\n",
        "y_out = layers.Dense(len(LABEL_COLUMNS), activation='sigmoid')(x)\n",
        "## compile\n",
        "model = models.Model(x_in, y_out)\n",
        "# bp_mll_loss\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam', metrics=['accuracy'])\n",
        "# model.compile(loss='bp_mll_loss',\n",
        "#               optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CkjLn4uDBlb"
      },
      "outputs": [],
      "source": [
        "y_train = train_df[LABEL_COLUMNS].values\n",
        "y_test = test_df[LABEL_COLUMNS].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHYsCZLUEeov"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-HU0lmADxtc"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1o7cOriP1lVY",
        "outputId": "dce7c292-ef1e-49dc-974f-7c695d555573"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "W2V_MODEL_PATH = \"YOUR_SAVE_MODEL\"\n",
        "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min')\n",
        "mcp_save = ModelCheckpoint(W2V_MODEL_PATH, save_best_only=True, monitor='val_loss', mode='min')\n",
        "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=1, epsilon=1e-4, mode='min')\n",
        "\n",
        "callbacks=[earlyStopping, mcp_save, reduce_lr_loss]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3a3fm6jcDzJJ"
      },
      "outputs": [],
      "source": [
        "## encode y\n",
        "# dic_y_mapping = {n:label for n,label in \n",
        "#                  enumerate(np.unique(y_train))}\n",
        "# inverse_dic = {v:k for k,v in dic_y_mapping.items()}\n",
        "# y_train = np.array([inverse_dic[y] for y in y_train])\n",
        "## train\n",
        "# training = model.fit(x=X_train, y=y_train, batch_size=256, \n",
        "#                      epochs=10, shuffle=True, verbose=0, \n",
        "#                      validation_split=0.3)\n",
        "training = model.fit(x=X_train, y=y_train, batch_size=128, \n",
        "                     epochs=50, shuffle=True, callbacks=callbacks, verbose=1,\n",
        "                     validation_data=(X_test, y_test))\n",
        "model.save_weights(W2V_MODEL_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZnt1xBV-FQr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uc3d0fvM9Ttz"
      },
      "outputs": [],
      "source": [
        "## plot loss and accuracy\n",
        "metrics = [k for k in training.history.keys() if (\"loss\" not in k) and (\"val\" not in k)]\n",
        "fig, ax = plt.subplots(nrows=1, ncols=2, sharey=True)\n",
        "ax[0].set(title=\"Training\")\n",
        "ax11 = ax[0].twinx()\n",
        "ax[0].plot(training.history['loss'], color='black')\n",
        "ax[0].set_xlabel('Epochs')\n",
        "ax[0].set_ylabel('Loss', color='black')\n",
        "for metric in metrics:\n",
        "    ax11.plot(training.history[metric], label=metric)\n",
        "ax11.set_ylabel(\"Score\", color='steelblue')\n",
        "ax11.legend()\n",
        "ax[1].set(title=\"Validation\")\n",
        "ax22 = ax[1].twinx()\n",
        "ax[1].plot(training.history['val_loss'], color='black')\n",
        "ax[1].set_xlabel('Epochs')\n",
        "ax[1].set_ylabel('Loss', color='black')\n",
        "for metric in metrics:\n",
        "     ax22.plot(training.history['val_'+metric], label=metric)\n",
        "ax22.set_ylabel(\"Score\", color=\"steelblue\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcBSNVSGFFEL"
      },
      "outputs": [],
      "source": [
        "# W2V_MODEL_PATH = \"SAVED_MODEL\"\n",
        "# model.save_weights(W2V_MODEL_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_prob = model.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxsueQqhCVP2",
        "outputId": "b6914684-f337-4809-8309-71580dd5a2aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33/33 [==============================] - 8s 163ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6y-lu3aF09-"
      },
      "outputs": [],
      "source": [
        "THRESHOLD = YOUR_THRESHOLD\n",
        "upper, lower = 1, 0\n",
        "\n",
        "y_pred = np.where(predicted_prob > THRESHOLD, upper, lower)\n",
        "\n",
        "print(classification_report(\n",
        "  y_test, \n",
        "  y_pred, \n",
        "  target_names=LABEL_COLUMNS, \n",
        "  zero_division=0\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hamming_score(y_pred, y_test)"
      ],
      "metadata": {
        "id": "w-rzrrBRy4nO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I5imdPHJBRVY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "cqiBwDNamMNh"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}